{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b1hwvZR6AWNe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-24 12:48:38.325919: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-05-24 12:48:38.330876: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-05-24 12:48:38.343216: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748071118.364052   22274 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748071118.370194   22274 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1748071118.385611   22274 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748071118.385636   22274 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748071118.385638   22274 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748071118.385639   22274 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-05-24 12:48:38.390198: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0WBvIo3KAcWD"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm1HQQKxBwg5",
        "outputId": "060d8399-e5fd-4612-f333-f7aa510e5f25"
      },
      "outputs": [],
      "source": [
        "(X_train_full,y_train_full),(X_test,y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "2bcWF2XLB2V8",
        "outputId": "7629562f-2f84-4bf7-be8c-53029bf109f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_full.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSQGa_-iDEzO"
      },
      "source": [
        "The images available here ranges from 0 to 255 pixels. Since we are about to train neural network it is important for us to scale down the images between 0 and 1\n",
        "We scale down neural network inputs for the following reasons:\n",
        "1. **Faster Convergence**: Gradient Descent optimizes the model by iteratively adjusting parameters based on gradients. If features are not scaled, larger values could lead to large gradient steps, causing the algorithm to overshoot the minimum, while smaller values may slow down convergence. Scaling ensures that all features contribute equally to the optimization process, speeding it up.\n",
        "2. **Numerical Stability**: Neural networks involve a lot of numerical computations. If input values are very large, it may result in numerical instability, like overflow or underflow, especially with certain activation functions. Scaling keeps the values within a manageable range.\n",
        "3. **Uniform Feature Influence**: Pixel intensities range from 0 to 255. By scaling them to the 0–1 range, you ensure that no particular feature (like a brighter pixel) disproportionately influences the model's weights, allowing the network to focus on relative patterns rather than absolute magnitudes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sq_CJ8aGCLAW"
      },
      "outputs": [],
      "source": [
        "#We will start by creating a validation set for our dataset\n",
        "X_valid, X_train = X_train_full[:5000]/255, X_train_full[5000:]/255\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gN3Az2HHCVR0"
      },
      "outputs": [],
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Jowy_luyFiIa",
        "outputId": "fe9c0ed1-61e4-4dcf-b956-bc6179b8c876"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Coat'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_names[y_train[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrBdbvzXFug6"
      },
      "source": [
        "# Building a neural network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRpvx2GpG-BM"
      },
      "source": [
        "A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2wYG903FnHe",
        "outputId": "39ea284f-6a87-4629-fd8c-e38822457468"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "2025-05-24 12:48:41.522214: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape = [28,28]))\n",
        "model.add(keras.layers.Dense(300,activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(100,activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(10,activation = \"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vevi82OIV7_"
      },
      "source": [
        "Going through the code line by line:\n",
        "1. The first line creates a Sqauential model. It is the simplest kind of model for keras neural network that are composed of a single stack of layers connected sequentially.\n",
        "\n",
        "2. Next we build our first layer and add it to our model. It is a flatten layer whose role is to convert each input image into a 1D array: It recieves input X and computes X.reshape(-1,1)\n",
        "\n",
        "3. Next we add a dense layer of 300 neurons. It will use relu Activation function. Each dense layer manages its own weight matrix, containing all the connected weights between the neuron and their inputs. It also has a vector bias term(one per neuron)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "DegiNPzDIFwE",
        "outputId": "2cb4ed3b-4a72-4508-e9ff-fbae716792a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2xitNKDK-Af"
      },
      "source": [
        "Dense layers have a lor of parameters. The first dense layer has 784*300 connected weight + 300 additional bias weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJcNohWDILr4",
        "outputId": "5af78497-6c19-4ea6-8a07-afa3428adc42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<Flatten name=flatten, built=True>,\n",
              " <Dense name=dense, built=True>,\n",
              " <Dense name=dense_1, built=True>,\n",
              " <Dense name=dense_2, built=True>]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.layers #getting model's list of layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt7mxgYRxbfa"
      },
      "source": [
        "All the parameters of a layer can be accessedusing get_weights() ans set_weights() methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "njup6-GBK7FB"
      },
      "outputs": [],
      "source": [
        "weights, biases = model.layers[2].get_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxI9rYuXyuJj",
        "outputId": "c7fb26a3-c5f9-4559-d957-5cf2289ba160"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.02624554, -0.09815837, -0.11795657, ...,  0.1055891 ,\n",
              "        -0.1164193 , -0.11082524],\n",
              "       [ 0.06422082, -0.10970664, -0.11457644, ..., -0.00041064,\n",
              "        -0.09663299, -0.00545229],\n",
              "       [-0.11581243, -0.05096698, -0.06017056, ..., -0.01162945,\n",
              "        -0.11493433, -0.01548717],\n",
              "       ...,\n",
              "       [ 0.04141504,  0.01964924,  0.01269581, ...,  0.05198155,\n",
              "        -0.09930325, -0.11056723],\n",
              "       [-0.11037203,  0.05966621,  0.03058437, ..., -0.04639397,\n",
              "        -0.084733  ,  0.09622457],\n",
              "       [-0.0106329 , -0.08736043, -0.08627223, ...,  0.08847349,\n",
              "         0.07527062, -0.11469004]], dtype=float32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axfjZj8fzbWn",
        "outputId": "468cd222-bd31-43a3-fc9a-21af9a334dd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnpTKRrVzvoo"
      },
      "source": [
        "If you want to use another initiliazition method, you can set kernal_initializer(weights initializer) or bias_initializer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlwWGBnJ0wUH"
      },
      "source": [
        "---\n",
        "The shape of weighgt matrix depends on the number of inputs. That is why it is recommended to specify input_shape when creating the first layer of sequential model.\n",
        "If you do not specify the input shape, Keras will simply weight until it knows the input shape before it builds the model. This will happen when you feed the data or when you call its build() method\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3Wxcy3y1Wat"
      },
      "source": [
        "After the model is created you must call its compile() function to specify the loss function and the optimizer to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lHq2IU1Dzcjm"
      },
      "outputs": [],
      "source": [
        "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"sgd\",metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7UHPdb3LxWE"
      },
      "source": [
        "Breakdown of each part:\n",
        "1. **Loss Function (`loss = \"sparse_categorical_crossentropy\"`):**  \n",
        "   The loss function is essentially a measure of how far off your model's predictions are from the actual labels. In this case, `\"sparse_categorical_crossentropy\"` is used, which is particularly suited for multi-class classification problems where the target labels are provided as integers (for example, 0, 1, 2, ...). Unlike the usual categorical crossentropy (which requires one-hot encoded labels), the \"sparse\" version saves memory and simplifies data preparation because it deals directly with integer labels.\n",
        "\n",
        "2. **Optimizer (`optimizer = \"sgd\"`):**  \n",
        "   The optimizer is the algorithm that adjusts the weights of your model in order to minimize the loss function. Here, `\"sgd\"` stands for Stochastic Gradient Descent. It works by iteratively updating the model's parameters in the direction that minimizes the loss based on the gradient computed from a subset (or batch) of the training data.\n",
        "\n",
        "3. **Metrics (`metrics = [\"accuracy\"]`):**  \n",
        "   Metrics are used to monitor the performance of your model during training and evaluation. By specifying `\"accuracy\"`, you're instructing the model to track the proportion of predictions that match the true labels. Accuracy is one of the most intuitive and common metrics for classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLqF-we_O8Cg"
      },
      "source": [
        "# Training and evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWisIFNqs0Va"
      },
      "source": [
        "fit() method also accepts callback that will let you specify a list of object that keras will call atthe end of training, at the start of each epoch and even before processing each batch. For example ModelCheckpoint callback saves checkpoint checkpoint of your model at regular intervals during training by default at the end of each epoch. Moreover if you use a validation set during training, you can set save_best_only=True when creating the ModelCheckpoint. In this case, it will only\n",
        " save your model when its performance on the validation set is the best so far. This\n",
        " way, you do not need to worry about training for too long and overfitting the training\n",
        " set: simply restore the last model saved after training\n",
        "\n",
        " Another way to implement early stopping is to simply use EarlyStopping callback. It will intrupt training when it measures no progress on the validation set for a number of epochs(defined by patience argument) and will optimally roll out the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGVpSaag1_af",
        "outputId": "0faa20c8-101d-4a75-c6f6-9727bf1d7942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-24 12:48:41.848652: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 172480000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6995 - loss: 0.9712 - val_accuracy: 0.8192 - val_loss: 0.5183\n",
            "Epoch 2/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8260 - loss: 0.4962 - val_accuracy: 0.8534 - val_loss: 0.4394\n",
            "Epoch 3/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8438 - loss: 0.4508 - val_accuracy: 0.8500 - val_loss: 0.4322\n",
            "Epoch 4/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8519 - loss: 0.4185 - val_accuracy: 0.8688 - val_loss: 0.3887\n",
            "Epoch 5/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8605 - loss: 0.4026 - val_accuracy: 0.8700 - val_loss: 0.3881\n",
            "Epoch 6/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8669 - loss: 0.3771 - val_accuracy: 0.8746 - val_loss: 0.3664\n",
            "Epoch 7/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8725 - loss: 0.3670 - val_accuracy: 0.8704 - val_loss: 0.3721\n",
            "Epoch 8/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8738 - loss: 0.3600 - val_accuracy: 0.8806 - val_loss: 0.3494\n",
            "Epoch 9/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8788 - loss: 0.3396 - val_accuracy: 0.8784 - val_loss: 0.3501\n",
            "Epoch 10/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8770 - loss: 0.3401 - val_accuracy: 0.8660 - val_loss: 0.3785\n",
            "Epoch 11/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8835 - loss: 0.3246 - val_accuracy: 0.8782 - val_loss: 0.3480\n",
            "Epoch 12/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8873 - loss: 0.3166 - val_accuracy: 0.8798 - val_loss: 0.3353\n",
            "Epoch 13/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8901 - loss: 0.3069 - val_accuracy: 0.8856 - val_loss: 0.3232\n",
            "Epoch 14/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8930 - loss: 0.2990 - val_accuracy: 0.8856 - val_loss: 0.3212\n",
            "Epoch 15/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8926 - loss: 0.2969 - val_accuracy: 0.8848 - val_loss: 0.3194\n",
            "Epoch 16/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8948 - loss: 0.2922 - val_accuracy: 0.8862 - val_loss: 0.3215\n",
            "Epoch 17/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8989 - loss: 0.2820 - val_accuracy: 0.8792 - val_loss: 0.3311\n",
            "Epoch 18/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9003 - loss: 0.2768 - val_accuracy: 0.8916 - val_loss: 0.3090\n",
            "Epoch 19/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9013 - loss: 0.2739 - val_accuracy: 0.8922 - val_loss: 0.3042\n",
            "Epoch 20/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9035 - loss: 0.2673 - val_accuracy: 0.8950 - val_loss: 0.3007\n",
            "Epoch 21/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9065 - loss: 0.2574 - val_accuracy: 0.8908 - val_loss: 0.3138\n",
            "Epoch 22/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9084 - loss: 0.2560 - val_accuracy: 0.8844 - val_loss: 0.3202\n",
            "Epoch 23/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9085 - loss: 0.2520 - val_accuracy: 0.8860 - val_loss: 0.3146\n",
            "Epoch 24/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9085 - loss: 0.2508 - val_accuracy: 0.8922 - val_loss: 0.2974\n",
            "Epoch 25/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9103 - loss: 0.2461 - val_accuracy: 0.8904 - val_loss: 0.3027\n",
            "Epoch 26/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9122 - loss: 0.2403 - val_accuracy: 0.8776 - val_loss: 0.3270\n",
            "Epoch 27/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9118 - loss: 0.2432 - val_accuracy: 0.8826 - val_loss: 0.3166\n",
            "Epoch 28/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9187 - loss: 0.2302 - val_accuracy: 0.8892 - val_loss: 0.3049\n",
            "Epoch 29/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9175 - loss: 0.2296 - val_accuracy: 0.8906 - val_loss: 0.2931\n",
            "Epoch 30/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9194 - loss: 0.2243 - val_accuracy: 0.8932 - val_loss: 0.3008\n",
            "Epoch 31/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9205 - loss: 0.2216 - val_accuracy: 0.8858 - val_loss: 0.3202\n",
            "Epoch 32/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9244 - loss: 0.2136 - val_accuracy: 0.8960 - val_loss: 0.2871\n",
            "Epoch 33/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9211 - loss: 0.2166 - val_accuracy: 0.8896 - val_loss: 0.3035\n",
            "Epoch 34/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9226 - loss: 0.2170 - val_accuracy: 0.8920 - val_loss: 0.3036\n",
            "Epoch 35/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9224 - loss: 0.2102 - val_accuracy: 0.8934 - val_loss: 0.2930\n",
            "Epoch 36/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9266 - loss: 0.2044 - val_accuracy: 0.8950 - val_loss: 0.2872\n",
            "Epoch 37/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9262 - loss: 0.2030 - val_accuracy: 0.8960 - val_loss: 0.2866\n",
            "Epoch 38/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9292 - loss: 0.1957 - val_accuracy: 0.8980 - val_loss: 0.2830\n",
            "Epoch 39/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9299 - loss: 0.1958 - val_accuracy: 0.8962 - val_loss: 0.2849\n",
            "Epoch 40/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9323 - loss: 0.1912 - val_accuracy: 0.8884 - val_loss: 0.3163\n",
            "Epoch 41/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9320 - loss: 0.1896 - val_accuracy: 0.8954 - val_loss: 0.2865\n",
            "Epoch 42/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9332 - loss: 0.1862 - val_accuracy: 0.8916 - val_loss: 0.2922\n",
            "Epoch 43/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9338 - loss: 0.1848 - val_accuracy: 0.8874 - val_loss: 0.3039\n",
            "Epoch 44/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.9357 - loss: 0.1788 - val_accuracy: 0.8958 - val_loss: 0.2951\n",
            "Epoch 45/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9367 - loss: 0.1772 - val_accuracy: 0.8954 - val_loss: 0.2984\n",
            "Epoch 46/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9386 - loss: 0.1732 - val_accuracy: 0.8894 - val_loss: 0.3228\n",
            "Epoch 47/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9404 - loss: 0.1709 - val_accuracy: 0.8918 - val_loss: 0.3038\n",
            "Epoch 48/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9404 - loss: 0.1675 - val_accuracy: 0.8980 - val_loss: 0.2850\n"
          ]
        }
      ],
      "source": [
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.keras\",save_best_only = True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\n",
        "history = model.fit(X_train,y_train,epochs = 100,validation_data = (X_valid,y_valid),callbacks = [checkpoint_cb,early_stopping_cb])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P-EwvHhT3Yl"
      },
      "source": [
        "---\n",
        "If the class weight was skewed, with some class being overrepresented and underrepresentd, It would be useful to set class_weight argument when calling the fit function, which would give larger weight to underrepresented classes and lower weight to overrepresented classes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGoI-dY6P54B",
        "outputId": "6d11a6ba-c2ec-4234-c543-04fcedb8d154"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'verbose': 'auto', 'epochs': 100, 'steps': 1719}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L29NNkKKVS9C",
        "outputId": "166cb4bd-cc16-4992-fc7b-4b1da470a7f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history.epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpKj1xKpVcOD",
        "outputId": "9559266e-7f20-43a8-e154-ffbe2ca48a92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': [0.772563636302948,\n",
              "  0.8306000232696533,\n",
              "  0.845727264881134,\n",
              "  0.8538363575935364,\n",
              "  0.861018180847168,\n",
              "  0.8653272986412048,\n",
              "  0.8720545172691345,\n",
              "  0.8739091157913208,\n",
              "  0.8773818016052246,\n",
              "  0.8796545267105103,\n",
              "  0.8822545409202576,\n",
              "  0.8870909214019775,\n",
              "  0.8886545300483704,\n",
              "  0.8910727500915527,\n",
              "  0.8923636078834534,\n",
              "  0.8948181867599487,\n",
              "  0.8976181745529175,\n",
              "  0.899290919303894,\n",
              "  0.9014182090759277,\n",
              "  0.9028545618057251,\n",
              "  0.9056727290153503,\n",
              "  0.9064182043075562,\n",
              "  0.9072909355163574,\n",
              "  0.9087272882461548,\n",
              "  0.9109272956848145,\n",
              "  0.9121999740600586,\n",
              "  0.9142909049987793,\n",
              "  0.9160545468330383,\n",
              "  0.9168545603752136,\n",
              "  0.9187090992927551,\n",
              "  0.9190363883972168,\n",
              "  0.921818196773529,\n",
              "  0.9214909076690674,\n",
              "  0.923836350440979,\n",
              "  0.9236545562744141,\n",
              "  0.925709068775177,\n",
              "  0.9270545244216919,\n",
              "  0.9288181662559509,\n",
              "  0.9292908906936646,\n",
              "  0.9312363862991333,\n",
              "  0.9316545724868774,\n",
              "  0.9334545731544495,\n",
              "  0.9337454438209534,\n",
              "  0.9347272515296936,\n",
              "  0.9356545209884644,\n",
              "  0.9367272853851318,\n",
              "  0.9397090673446655,\n",
              "  0.9391818046569824],\n",
              " 'loss': [0.7032063007354736,\n",
              "  0.48582661151885986,\n",
              "  0.4416421949863434,\n",
              "  0.4156695604324341,\n",
              "  0.39727550745010376,\n",
              "  0.38072335720062256,\n",
              "  0.3662061095237732,\n",
              "  0.355969101190567,\n",
              "  0.34390681982040405,\n",
              "  0.3351806700229645,\n",
              "  0.3266158401966095,\n",
              "  0.31738024950027466,\n",
              "  0.3108740448951721,\n",
              "  0.30324527621269226,\n",
              "  0.2970394492149353,\n",
              "  0.29069438576698303,\n",
              "  0.2849774658679962,\n",
              "  0.27975451946258545,\n",
              "  0.27312323451042175,\n",
              "  0.26842832565307617,\n",
              "  0.26332902908325195,\n",
              "  0.258995920419693,\n",
              "  0.25474846363067627,\n",
              "  0.2507147490978241,\n",
              "  0.24593804776668549,\n",
              "  0.24195538461208344,\n",
              "  0.2383749783039093,\n",
              "  0.23410703241825104,\n",
              "  0.23119081556797028,\n",
              "  0.2260272055864334,\n",
              "  0.22276166081428528,\n",
              "  0.2194824516773224,\n",
              "  0.21569083631038666,\n",
              "  0.2132827639579773,\n",
              "  0.20816203951835632,\n",
              "  0.206130713224411,\n",
              "  0.2029731124639511,\n",
              "  0.19888190925121307,\n",
              "  0.1961507350206375,\n",
              "  0.19320279359817505,\n",
              "  0.19132962822914124,\n",
              "  0.1873268336057663,\n",
              "  0.1843682825565338,\n",
              "  0.18178755044937134,\n",
              "  0.17916609346866608,\n",
              "  0.17556874454021454,\n",
              "  0.17198842763900757,\n",
              "  0.17030803859233856],\n",
              " 'val_accuracy': [0.8191999793052673,\n",
              "  0.8533999919891357,\n",
              "  0.8500000238418579,\n",
              "  0.8687999844551086,\n",
              "  0.8700000047683716,\n",
              "  0.8745999932289124,\n",
              "  0.8704000115394592,\n",
              "  0.8805999755859375,\n",
              "  0.8784000277519226,\n",
              "  0.8659999966621399,\n",
              "  0.8781999945640564,\n",
              "  0.879800021648407,\n",
              "  0.8855999708175659,\n",
              "  0.8855999708175659,\n",
              "  0.8848000168800354,\n",
              "  0.8862000107765198,\n",
              "  0.8791999816894531,\n",
              "  0.8916000127792358,\n",
              "  0.8921999931335449,\n",
              "  0.8949999809265137,\n",
              "  0.8907999992370605,\n",
              "  0.8844000101089478,\n",
              "  0.8859999775886536,\n",
              "  0.8921999931335449,\n",
              "  0.8903999924659729,\n",
              "  0.8776000142097473,\n",
              "  0.8826000094413757,\n",
              "  0.88919997215271,\n",
              "  0.8906000256538391,\n",
              "  0.8931999802589417,\n",
              "  0.8858000040054321,\n",
              "  0.8960000276565552,\n",
              "  0.8895999789237976,\n",
              "  0.8920000195503235,\n",
              "  0.8934000134468079,\n",
              "  0.8949999809265137,\n",
              "  0.8960000276565552,\n",
              "  0.8980000019073486,\n",
              "  0.8962000012397766,\n",
              "  0.8884000182151794,\n",
              "  0.8953999876976013,\n",
              "  0.8916000127792358,\n",
              "  0.8873999714851379,\n",
              "  0.895799994468689,\n",
              "  0.8953999876976013,\n",
              "  0.8894000053405762,\n",
              "  0.8917999863624573,\n",
              "  0.8980000019073486],\n",
              " 'val_loss': [0.5183367729187012,\n",
              "  0.43944913148880005,\n",
              "  0.43224233388900757,\n",
              "  0.3887179493904114,\n",
              "  0.38808804750442505,\n",
              "  0.3663831055164337,\n",
              "  0.37209171056747437,\n",
              "  0.34935376048088074,\n",
              "  0.3501139283180237,\n",
              "  0.3785322904586792,\n",
              "  0.34802836179733276,\n",
              "  0.33528414368629456,\n",
              "  0.3232066035270691,\n",
              "  0.32124605774879456,\n",
              "  0.3194248676300049,\n",
              "  0.3215445280075073,\n",
              "  0.33110010623931885,\n",
              "  0.3090020716190338,\n",
              "  0.30423954129219055,\n",
              "  0.30070656538009644,\n",
              "  0.3138215243816376,\n",
              "  0.32020801305770874,\n",
              "  0.3145952522754669,\n",
              "  0.2974454164505005,\n",
              "  0.302676260471344,\n",
              "  0.3270252048969269,\n",
              "  0.31662407517433167,\n",
              "  0.30486559867858887,\n",
              "  0.2931486964225769,\n",
              "  0.30077439546585083,\n",
              "  0.3202436566352844,\n",
              "  0.2870594561100006,\n",
              "  0.3035430312156677,\n",
              "  0.30362269282341003,\n",
              "  0.29299435019493103,\n",
              "  0.2871932089328766,\n",
              "  0.2865648865699768,\n",
              "  0.2829759418964386,\n",
              "  0.2849195897579193,\n",
              "  0.3162928819656372,\n",
              "  0.2864966094493866,\n",
              "  0.29219186305999756,\n",
              "  0.3039109706878662,\n",
              "  0.29509857296943665,\n",
              "  0.2983912229537964,\n",
              "  0.322833776473999,\n",
              "  0.3037875294685364,\n",
              "  0.28497645258903503]}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zrYMAwlyVgfS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "NK3lnA5AVvag",
        "outputId": "660ed78e-8fa6-4441-84d6-d0369af2a368"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pd.DataFrame(history.history).plot(figsize = (8,5))\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9G8v8rmV15j",
        "outputId": "471ed2a5-8497-42a5-a1d6-7cf70b580bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8894 - loss: 0.3220\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.31877994537353516, 0.8899000287055969]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(X_test/255,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gkAsRyXrXuvO"
      },
      "outputs": [],
      "source": [
        "model.save(\"my_keras_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"my_keras_model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOGkO_jWy6qc"
      },
      "source": [
        "# Fine tuining neural network yperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546x2ZgmzCPC"
      },
      "source": [
        "---\n",
        "What does it mean to fine tune neural network Hyperparameters:\n",
        "Fine-tuning neural network hyperparameters refers to the process of optimizing the parameters that control the training and architecture of a neural network to achieve better performance on a given task. Hyperparameters are not learned during training; instead, they are set prior to training and can significantly impact the model's accuracy, efficiency, and generalization.\n",
        "\n",
        "Common Hyperparameters to Fine-Tune:\n",
        "\n",
        "1.Learning Rate: Determines how much the weights are updated during training. A suitable learning rate ensures faster convergence without overshooting the optimal solution.\n",
        "\n",
        "\n",
        "2.Batch Size: Specifies the number of samples processed before updating the model's parameters. Smaller batch sizes provide noisy updates, while larger ones offer stability but require more memory.\n",
        "\n",
        "\n",
        "3.Number of Layers and Neurons: Defines the depth and width of the network, impacting its capacity to learn complex patterns.\n",
        "\n",
        "\n",
        "4.Activation Functions: Controls how neurons output values, influencing non-linear learning capabilities.\n",
        "\n",
        "\n",
        "5.Dropout Rate: Prevents overfitting by randomly ignoring certain neurons during training.\n",
        "\n",
        "\n",
        "6.Optimizer: Algorithms like SGD, Adam, or RMSprop determine how weights are updated based on gradients.\n",
        "\n",
        "\n",
        "7.Epochs: The number of complete passes through the training dataset.\n",
        "\n",
        "\n",
        "Techniques for Fine-Tuning:\n",
        "\n",
        "1.Grid Search: Tests combinations of hyperparameters systematically across a predefined range.\n",
        "\n",
        "\n",
        "2.Random Search: Samples random combinations of hyperparameters within specified ranges.\n",
        "\n",
        "\n",
        "3.Bayesian Optimization: Uses probabilistic models to predict promising hyperparameter configurations.\n",
        "\n",
        "\n",
        "4.Manual Tuning: Adjusting hyperparameters based on domain knowledge and empirical results.\n",
        "\n",
        "\n",
        "5.Fine-tuning is critical for achieving optimal model performance while balancing computational efficiency and generalization ability.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqUggCbP14Gp"
      },
      "source": [
        "Inorder to use hyperparameter features from scikit-learn we need to wrap our keras model into object so that it can mimic regular scikit-learn regerssors.\n",
        "\n",
        "\n",
        "The first step is to create a function that will build and compile a keras model, given its set of hyperparameters\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "U8b6WDfA1oJe"
      },
      "outputs": [],
      "source": [
        "def build_model(n_hidden = 1, n_neurons = 30, learning_rate = 0.003,input_shape = [28,28]):\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape= input_shape))\n",
        "  for layer in range(n_hidden):\n",
        "    model.add(keras.layers.Dense(n_neurons,activation = \"relu\"))\n",
        "  model.add(keras.layers.Dense(1))\n",
        "  optimizer = keras.optimizers.SGD(learning_rate = learning_rate)\n",
        "  model.compile(loss = \"mse\",optimizer = optimizer)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pR-VsHc6a5y"
      },
      "source": [
        "This is a simple model for univariate regression( only one output neuron).\n",
        "Now we create a KerasRegressor() on this build_model() function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "7Oiu1vpJ5wNG",
        "outputId": "1b002e4b-ca2e-4a29-b627-a915be178ef8"
      },
      "outputs": [],
      "source": [
        "from scikeras.wrappers import KerasRegressor\n",
        "keras_reg = KerasRegressor(build_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KCK288lA87V1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-24 12:53:49.753290: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 172480000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 3.2861 - val_loss: 1.8347\n",
            "Epoch 2/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.8582 - val_loss: 1.6515\n",
            "Epoch 3/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.7016 - val_loss: 1.6208\n",
            "Epoch 4/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.5958 - val_loss: 1.5745\n",
            "Epoch 5/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.5313 - val_loss: 1.6611\n",
            "Epoch 6/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.5003 - val_loss: 1.4625\n",
            "Epoch 7/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.4673 - val_loss: 1.4289\n",
            "Epoch 8/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.4588 - val_loss: 1.4238\n",
            "Epoch 9/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.4091 - val_loss: 1.4264\n",
            "Epoch 10/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 1.3716 - val_loss: 1.4590\n",
            "Epoch 11/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 1.3633 - val_loss: 1.4015\n",
            "Epoch 12/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.3425 - val_loss: 1.3760\n",
            "Epoch 13/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 1.3121 - val_loss: 1.4059\n",
            "Epoch 14/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 1.3003 - val_loss: 1.3642\n",
            "Epoch 15/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.2958 - val_loss: 1.3399\n",
            "Epoch 16/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.2889 - val_loss: 1.4877\n",
            "Epoch 17/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.2835 - val_loss: 1.3145\n",
            "Epoch 18/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.2523 - val_loss: 1.3202\n",
            "Epoch 19/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.2588 - val_loss: 1.3471\n",
            "Epoch 20/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.2165 - val_loss: 1.5146\n",
            "Epoch 21/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.2438 - val_loss: 1.3624\n",
            "Epoch 22/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.2036 - val_loss: 1.2939\n",
            "Epoch 23/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 1.2055 - val_loss: 1.3109\n",
            "Epoch 24/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 1.1997 - val_loss: 1.2855\n",
            "Epoch 25/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.1935 - val_loss: 1.3434\n",
            "Epoch 26/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1779 - val_loss: 1.3112\n",
            "Epoch 27/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1769 - val_loss: 1.3284\n",
            "Epoch 28/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1543 - val_loss: 1.3049\n",
            "Epoch 29/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1417 - val_loss: 1.2779\n",
            "Epoch 30/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1429 - val_loss: 1.2737\n",
            "Epoch 31/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1293 - val_loss: 1.3560\n",
            "Epoch 32/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 1.1437 - val_loss: 1.2773\n",
            "Epoch 33/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 1.1359 - val_loss: 1.2469\n",
            "Epoch 34/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1091 - val_loss: 1.2349\n",
            "Epoch 35/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.1007 - val_loss: 1.3260\n",
            "Epoch 36/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0990 - val_loss: 1.2486\n",
            "Epoch 37/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 1.1277 - val_loss: 1.3340\n",
            "Epoch 38/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.0659 - val_loss: 1.2332\n",
            "Epoch 39/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 1.0886 - val_loss: 1.2587\n",
            "Epoch 40/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0790 - val_loss: 1.2791\n",
            "Epoch 41/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.0555 - val_loss: 1.3364\n",
            "Epoch 42/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0840 - val_loss: 1.2677\n",
            "Epoch 43/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0622 - val_loss: 1.2939\n",
            "Epoch 44/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.0668 - val_loss: 1.2355\n",
            "Epoch 45/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0214 - val_loss: 1.2185\n",
            "Epoch 46/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 1.0211 - val_loss: 1.2119\n",
            "Epoch 47/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 1.0332 - val_loss: 1.2086\n",
            "Epoch 48/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 1.0493 - val_loss: 1.2425\n",
            "Epoch 49/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.0184 - val_loss: 1.3870\n",
            "Epoch 50/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0149 - val_loss: 1.2360\n",
            "Epoch 51/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.0249 - val_loss: 1.2585\n",
            "Epoch 52/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.0163 - val_loss: 1.2531\n",
            "Epoch 53/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0160 - val_loss: 1.2173\n",
            "Epoch 54/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0271 - val_loss: 1.2102\n",
            "Epoch 55/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 1.0038 - val_loss: 1.2552\n",
            "Epoch 56/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 1.0034 - val_loss: 1.2666\n",
            "Epoch 57/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9838 - val_loss: 1.2308\n",
            "Epoch 58/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9800 - val_loss: 1.2259\n",
            "Epoch 59/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9820 - val_loss: 1.2313\n",
            "Epoch 60/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9722 - val_loss: 1.2898\n",
            "Epoch 61/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9867 - val_loss: 1.2800\n",
            "Epoch 62/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9826 - val_loss: 1.2243\n",
            "Epoch 63/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9751 - val_loss: 1.2312\n",
            "Epoch 64/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9861 - val_loss: 1.2196\n",
            "Epoch 65/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9405 - val_loss: 1.2154\n",
            "Epoch 66/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9621 - val_loss: 1.2224\n",
            "Epoch 67/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9514 - val_loss: 1.2509\n",
            "Epoch 68/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9601 - val_loss: 1.2161\n",
            "Epoch 69/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9641 - val_loss: 1.3221\n",
            "Epoch 70/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9406 - val_loss: 1.2419\n",
            "Epoch 71/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9498 - val_loss: 1.2076\n",
            "Epoch 72/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9166 - val_loss: 1.2717\n",
            "Epoch 73/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9377 - val_loss: 1.2281\n",
            "Epoch 74/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9338 - val_loss: 1.2080\n",
            "Epoch 75/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9376 - val_loss: 1.2224\n",
            "Epoch 76/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9401 - val_loss: 1.2059\n",
            "Epoch 77/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9384 - val_loss: 1.2235\n",
            "Epoch 78/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9254 - val_loss: 1.2060\n",
            "Epoch 79/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9295 - val_loss: 1.2030\n",
            "Epoch 80/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9495 - val_loss: 1.3441\n",
            "Epoch 81/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9252 - val_loss: 1.1932\n",
            "Epoch 82/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9186 - val_loss: 1.2414\n",
            "Epoch 83/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9181 - val_loss: 1.2338\n",
            "Epoch 84/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9249 - val_loss: 1.3088\n",
            "Epoch 85/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9196 - val_loss: 1.2244\n",
            "Epoch 86/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.8910 - val_loss: 1.3083\n",
            "Epoch 87/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9008 - val_loss: 1.2316\n",
            "Epoch 88/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9319 - val_loss: 1.2320\n",
            "Epoch 89/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.8996 - val_loss: 1.2767\n",
            "Epoch 90/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.9071 - val_loss: 1.4087\n",
            "Epoch 91/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.8829 - val_loss: 1.2262\n",
            "Epoch 92/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.8913 - val_loss: 1.3418\n",
            "Epoch 93/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.8928 - val_loss: 1.2259\n",
            "Epoch 94/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.8758 - val_loss: 1.3218\n",
            "Epoch 95/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.8695 - val_loss: 1.2094\n",
            "Epoch 96/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.8738 - val_loss: 1.2630\n",
            "Epoch 97/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9086 - val_loss: 1.2104\n",
            "Epoch 98/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.8874 - val_loss: 1.2140\n",
            "Epoch 99/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.8736 - val_loss: 1.2464\n",
            "Epoch 100/100\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.8731 - val_loss: 1.2558\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
              "\tmodel=&lt;function build_model at 0x7d2377f06700&gt;\n",
              "\tbuild_fn=None\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=None\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=1\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=1\n",
              ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;KerasRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KerasRegressor(\n",
              "\tmodel=&lt;function build_model at 0x7d2377f06700&gt;\n",
              "\tbuild_fn=None\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=None\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=1\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=1\n",
              ")</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "KerasRegressor(\n",
              "\tmodel=<function build_model at 0x7d2377f06700>\n",
              "\tbuild_fn=None\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=None\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=1\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=1\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keras_reg.fit(X_train, y_train, epochs = 100,validation_data = (X_valid,y_valid),callbacks = [keras.callbacks.EarlyStopping(patience = 10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hdZx0fAjQ1XE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        }
      ],
      "source": [
        "mse_test = keras_reg.score(X_test/255,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8335518836975098"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mse_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fgOC9AGs9hjw"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs'])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keras_reg.get_params().keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-24 13:03:13.327858: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 114984576 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 4.7110 - val_loss: 2.9788\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.9311 - val_loss: 2.6205\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.6334 - val_loss: 2.4680\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.4168 - val_loss: 2.2956\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.3147 - val_loss: 2.2072\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.2423 - val_loss: 2.1578\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.1843 - val_loss: 2.1310\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.1478 - val_loss: 2.0725\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.0441 - val_loss: 2.0385\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0947 - val_loss: 2.0332\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.0008964307779758178, model__n_hidden=0, model__n_neurons=63; total time=  29.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-24 13:03:42.648683: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 114987712 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.9071 - val_loss: 2.9295\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.8279 - val_loss: 2.6161\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.6390 - val_loss: 2.4356\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.4221 - val_loss: 2.3105\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.3125 - val_loss: 2.2182\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.2136 - val_loss: 2.1756\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.1747 - val_loss: 2.1126\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.0771 - val_loss: 2.0729\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0596 - val_loss: 2.0482\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0424 - val_loss: 2.0265\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.0008964307779758178, model__n_hidden=0, model__n_neurons=63; total time=  28.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "2025-05-24 13:04:10.818153: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 114987712 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 4.9621 - val_loss: 2.9028\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.9062 - val_loss: 2.6008\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.6793 - val_loss: 2.4116\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.4622 - val_loss: 2.2889\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.3299 - val_loss: 2.2185\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.2341 - val_loss: 2.1440\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.1588 - val_loss: 2.0967\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.1026 - val_loss: 2.0599\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.0873 - val_loss: 2.0592\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0516 - val_loss: 2.0312\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.0008964307779758178, model__n_hidden=0, model__n_neurons=63; total time=  29.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.1233 - val_loss: 2.7084\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.6529 - val_loss: 2.3088\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.3422 - val_loss: 2.1199\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.0688 - val_loss: 1.9621\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 2.0021 - val_loss: 1.8783\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.9060 - val_loss: 1.8257\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.8235 - val_loss: 1.7855\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.8055 - val_loss: 1.7483\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.7678 - val_loss: 1.7263\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.7553 - val_loss: 1.7450\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.0005432621591177808, model__n_hidden=1, model__n_neurons=94; total time=  34.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.6510 - val_loss: 2.7515\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.6565 - val_loss: 2.3580\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.3519 - val_loss: 2.1447\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.1445 - val_loss: 2.0104\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.0139 - val_loss: 1.9303\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.9313 - val_loss: 1.8686\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.9217 - val_loss: 1.8310\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.8181 - val_loss: 1.8070\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.7519 - val_loss: 1.7708\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.7633 - val_loss: 1.7416\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "[CV] END model__learning_rate=0.0005432621591177808, model__n_hidden=1, model__n_neurons=94; total time=  35.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.9929 - val_loss: 2.6466\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.6041 - val_loss: 2.2664\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.3145 - val_loss: 2.1135\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.1011 - val_loss: 1.9492\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 2.0003 - val_loss: 1.8833\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.9124 - val_loss: 1.8332\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.8481 - val_loss: 1.8049\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.8360 - val_loss: 1.7951\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.8151 - val_loss: 1.7524\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.7870 - val_loss: 1.7250\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.0005432621591177808, model__n_hidden=1, model__n_neurons=94; total time=  36.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.9612 - val_loss: 1.6756\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.7223 - val_loss: 1.5083\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.5686 - val_loss: 1.4996\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.3932 - val_loss: 1.3426\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.3197 - val_loss: 1.3466\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.3307 - val_loss: 1.3610\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.2353 - val_loss: 1.3333\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.2164 - val_loss: 1.2798\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.1668 - val_loss: 1.2654\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1.1286 - val_loss: 1.2535\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "[CV] END model__learning_rate=0.024552263765862874, model__n_hidden=3, model__n_neurons=75; total time=  37.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3.7820 - val_loss: 1.6577\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.5998 - val_loss: 1.4379\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.4303 - val_loss: 1.3325\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 1.3201 - val_loss: 1.4017\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.2668 - val_loss: 1.2729\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.2538 - val_loss: 1.3433\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1910 - val_loss: 1.2855\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1710 - val_loss: 1.2666\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1091 - val_loss: 1.3035\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1169 - val_loss: 1.2868\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "[CV] END model__learning_rate=0.024552263765862874, model__n_hidden=3, model__n_neurons=75; total time=  33.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 6.0220 - val_loss: 1.5918\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.7057 - val_loss: 1.5305\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.5598 - val_loss: 1.5090\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.4785 - val_loss: 1.4164\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.3792 - val_loss: 1.4039\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.3152 - val_loss: 1.3112\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.2434 - val_loss: 1.3018\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.2136 - val_loss: 1.9125\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1906 - val_loss: 1.4408\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.2266 - val_loss: 1.2704\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.024552263765862874, model__n_hidden=3, model__n_neurons=75; total time=  31.4s\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 921us/step\n",
            "[CV] END model__learning_rate=0.01852780484051431, model__n_hidden=0, model__n_neurons=82; total time=  20.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1127, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1724, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 1204, in r2_score\n",
            "    _, y_true, y_pred, multioutput = _check_reg_targets(\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
            "    _assert_all_finite(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
            "    _assert_all_finite_element_wise(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
            "    raise ValueError(msg_err)\n",
            "ValueError: Input contains NaN.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 887us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1127, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1724, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 1204, in r2_score\n",
            "    _, y_true, y_pred, multioutput = _check_reg_targets(\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
            "    _assert_all_finite(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
            "    _assert_all_finite_element_wise(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
            "    raise ValueError(msg_err)\n",
            "ValueError: Input contains NaN.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END model__learning_rate=0.01852780484051431, model__n_hidden=0, model__n_neurons=82; total time=  26.1s\n",
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1127, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1724, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 1204, in r2_score\n",
            "    _, y_true, y_pred, multioutput = _check_reg_targets(\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
            "    _assert_all_finite(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
            "    _assert_all_finite_element_wise(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
            "    raise ValueError(msg_err)\n",
            "ValueError: Input contains NaN.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END model__learning_rate=0.01852780484051431, model__n_hidden=0, model__n_neurons=82; total time=  31.2s\n",
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 17.8996 - val_loss: 8.2301\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 8.3030 - val_loss: 8.2290\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 8.2900 - val_loss: 8.2294\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 8.3265 - val_loss: 8.2290\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 8.3262 - val_loss: 8.2287\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 8.2170 - val_loss: 7.9050\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 5.0723 - val_loss: 2.6079\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.5974 - val_loss: 2.3386\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.4052 - val_loss: 2.2316\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.2684 - val_loss: 2.1341\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.000695398961820738, model__n_hidden=2, model__n_neurons=3; total time=  31.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 7.3641 - val_loss: 2.3708\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.3708 - val_loss: 2.1531\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.1746 - val_loss: 2.0594\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.0409 - val_loss: 1.9450\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.9403 - val_loss: 1.8809\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.8377 - val_loss: 1.8696\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.8475 - val_loss: 1.8365\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.7631 - val_loss: 1.7892\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.7464 - val_loss: 1.7635\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.7699 - val_loss: 1.7554\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step\n",
            "[CV] END model__learning_rate=0.000695398961820738, model__n_hidden=2, model__n_neurons=3; total time=  28.7s\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 8.1719 - val_loss: 2.5035\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.5374 - val_loss: 2.2185\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.2316 - val_loss: 2.2350\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.1678 - val_loss: 1.9847\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.9838 - val_loss: 1.9307\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.9378 - val_loss: 1.8457\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.9078 - val_loss: 1.8493\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.8679 - val_loss: 1.8123\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.8569 - val_loss: 1.7844\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.8086 - val_loss: 1.7755\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "[CV] END model__learning_rate=0.000695398961820738, model__n_hidden=2, model__n_neurons=3; total time=  23.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 5.5067 - val_loss: 2.1099\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0645 - val_loss: 1.8676\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.8611 - val_loss: 1.7895\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.7586 - val_loss: 1.7234\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.7275 - val_loss: 1.7377\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.6582 - val_loss: 1.6188\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.5987 - val_loss: 1.5915\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.5761 - val_loss: 1.6373\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.5429 - val_loss: 1.5461\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.5112 - val_loss: 1.5291\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.001055672744031813, model__n_hidden=3, model__n_neurons=31; total time=  28.9s\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 5.3516 - val_loss: 2.1530\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0106 - val_loss: 2.0288\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.8451 - val_loss: 1.7599\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.7420 - val_loss: 1.6993\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.6564 - val_loss: 1.7383\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.5876 - val_loss: 1.6191\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.5312 - val_loss: 1.5323\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.5074 - val_loss: 1.5147\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.4970 - val_loss: 1.6092\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.4372 - val_loss: 1.6362\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "[CV] END model__learning_rate=0.001055672744031813, model__n_hidden=3, model__n_neurons=31; total time=  28.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 4.3902 - val_loss: 2.1186\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 2.0817 - val_loss: 1.8637\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.9021 - val_loss: 1.7616\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 1.7800 - val_loss: 1.6700\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.6641 - val_loss: 1.6099\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.5946 - val_loss: 1.6230\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.5628 - val_loss: 1.5407\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.5295 - val_loss: 1.5231\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.5091 - val_loss: 1.4888\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.4667 - val_loss: 1.4906\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.001055672744031813, model__n_hidden=3, model__n_neurons=31; total time=  41.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3.6258 - val_loss: 1.6404\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.6613 - val_loss: 1.5556\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.5060 - val_loss: 1.4247\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.3781 - val_loss: 1.3369\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.3396 - val_loss: 1.2887\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.2460 - val_loss: 1.6326\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.2307 - val_loss: 1.2530\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.1892 - val_loss: 1.2166\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.0998 - val_loss: 1.2416\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.1046 - val_loss: 1.2461\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "[CV] END model__learning_rate=0.008567615858822257, model__n_hidden=3, model__n_neurons=54; total time=  35.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3.3022 - val_loss: 1.7851\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.6144 - val_loss: 1.5181\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.4826 - val_loss: 1.5543\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.3724 - val_loss: 1.3412\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.2933 - val_loss: 1.3437\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.2407 - val_loss: 1.2355\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.1953 - val_loss: 1.3091\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.1488 - val_loss: 1.2068\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.1425 - val_loss: 1.1900\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.0452 - val_loss: 1.2642\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "[CV] END model__learning_rate=0.008567615858822257, model__n_hidden=3, model__n_neurons=54; total time=  37.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3.3387 - val_loss: 1.5606\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.6795 - val_loss: 1.5006\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.5168 - val_loss: 1.3741\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.3797 - val_loss: 1.3329\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.3257 - val_loss: 1.2613\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.2732 - val_loss: 1.2855\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.2062 - val_loss: 1.2542\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.1693 - val_loss: 1.3472\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1537 - val_loss: 1.3819\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0742 - val_loss: 1.2085\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.008567615858822257, model__n_hidden=3, model__n_neurons=54; total time=  35.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3.8622 - val_loss: 1.9371\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.9448 - val_loss: 1.8536\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.7665 - val_loss: 1.6858\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.6255 - val_loss: 1.6039\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.5709 - val_loss: 1.7495\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.5092 - val_loss: 1.5175\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.4815 - val_loss: 1.4927\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.4641 - val_loss: 1.4507\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.3967 - val_loss: 1.4597\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.3656 - val_loss: 1.4417\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.001760917126825205, model__n_hidden=2, model__n_neurons=53; total time=  28.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 3.8332 - val_loss: 2.0726\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.9789 - val_loss: 1.7651\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.7306 - val_loss: 1.6757\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.6812 - val_loss: 1.7039\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.6065 - val_loss: 1.5750\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.5067 - val_loss: 1.6694\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.5141 - val_loss: 1.4836\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.4530 - val_loss: 1.4724\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.4274 - val_loss: 1.4949\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.4272 - val_loss: 1.4252\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.001760917126825205, model__n_hidden=2, model__n_neurons=53; total time=  30.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3.6510 - val_loss: 1.9535\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.9750 - val_loss: 1.7954\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.7926 - val_loss: 1.6911\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.6552 - val_loss: 1.5892\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.5964 - val_loss: 1.7326\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.5909 - val_loss: 1.6332\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.5100 - val_loss: 1.5011\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.4362 - val_loss: 1.4723\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.4371 - val_loss: 1.4725\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.3919 - val_loss: 1.4579\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "[CV] END model__learning_rate=0.001760917126825205, model__n_hidden=2, model__n_neurons=53; total time=  31.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848us/step\n",
            "[CV] END model__learning_rate=0.01749158158537172, model__n_hidden=0, model__n_neurons=91; total time=  23.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1127, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1724, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 1204, in r2_score\n",
            "    _, y_true, y_pred, multioutput = _check_reg_targets(\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
            "    _assert_all_finite(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
            "    _assert_all_finite_element_wise(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
            "    raise ValueError(msg_err)\n",
            "ValueError: Input contains NaN.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1127, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1724, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 1204, in r2_score\n",
            "    _, y_true, y_pred, multioutput = _check_reg_targets(\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
            "    _assert_all_finite(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
            "    _assert_all_finite_element_wise(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
            "    raise ValueError(msg_err)\n",
            "ValueError: Input contains NaN.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END model__learning_rate=0.01749158158537172, model__n_hidden=0, model__n_neurons=91; total time=  26.2s\n",
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1127, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/scikeras/wrappers.py\", line 1724, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 1204, in r2_score\n",
            "    _, y_true, y_pred, multioutput = _check_reg_targets(\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
            "    _assert_all_finite(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
            "    _assert_all_finite_element_wise(\n",
            "  File \"/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
            "    raise ValueError(msg_err)\n",
            "ValueError: Input contains NaN.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END model__learning_rate=0.01749158158537172, model__n_hidden=0, model__n_neurons=91; total time=  33.0s\n",
            "Epoch 1/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 4.9001 - val_loss: 2.9429\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.9203 - val_loss: 2.6305\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.6122 - val_loss: 2.4465\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.4639 - val_loss: 2.3256\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.3328 - val_loss: 2.2302\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.2583 - val_loss: 2.1699\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.1837 - val_loss: 2.1125\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0825 - val_loss: 2.0815\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.1071 - val_loss: 2.0464\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0763 - val_loss: 2.0309\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 842us/step\n",
            "[CV] END model__learning_rate=0.0008491074147640596, model__n_hidden=0, model__n_neurons=75; total time=  22.4s\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 4.7191 - val_loss: 2.9464\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.9083 - val_loss: 2.6435\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.6132 - val_loss: 2.4536\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.4724 - val_loss: 2.3322\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.3486 - val_loss: 2.2388\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.2598 - val_loss: 2.2219\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.1666 - val_loss: 2.1229\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.1300 - val_loss: 2.0823\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0354 - val_loss: 2.0567\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0679 - val_loss: 2.0318\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step\n",
            "[CV] END model__learning_rate=0.0008491074147640596, model__n_hidden=0, model__n_neurons=75; total time=  21.5s\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 4.6708 - val_loss: 2.9358\n",
            "Epoch 2/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.9011 - val_loss: 2.6249\n",
            "Epoch 3/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.6199 - val_loss: 2.4618\n",
            "Epoch 4/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.4679 - val_loss: 2.3109\n",
            "Epoch 5/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.3779 - val_loss: 2.2292\n",
            "Epoch 6/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.2467 - val_loss: 2.1625\n",
            "Epoch 7/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.1993 - val_loss: 2.1110\n",
            "Epoch 8/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.1316 - val_loss: 2.0701\n",
            "Epoch 9/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 2.1317 - val_loss: 2.0450\n",
            "Epoch 10/10\n",
            "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.0866 - val_loss: 2.0348\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 913us/step\n",
            "[CV] END model__learning_rate=0.0008491074147640596, model__n_hidden=0, model__n_neurons=75; total time=  25.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.75000443 0.78590498 0.84630426        nan 0.76605769 0.81096594\n",
            " 0.84888643 0.8236028         nan 0.74947904]\n",
            "  warnings.warn(\n",
            "/home/soulking/Documents/Machine learning codes/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 3.1139 - val_loss: 1.5966\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.5228 - val_loss: 1.4095\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.3664 - val_loss: 1.3321\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.2686 - val_loss: 1.2457\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.2305 - val_loss: 1.2489\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1.1899 - val_loss: 1.2282\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.1030 - val_loss: 1.2135\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.0926 - val_loss: 1.2002\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.0631 - val_loss: 1.1747\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.0268 - val_loss: 1.1347\n",
            "Best parameters: {'model__learning_rate': np.float64(0.008567615858822257), 'model__n_hidden': 3, 'model__n_neurons': np.int64(54)}\n",
            "Best score: 0.8488864302635193\n"
          ]
        }
      ],
      "source": [
        "# Update param_distribution keys to use 'model__' prefix\n",
        "param_distribution_prefixed = {\n",
        "\t\"model__n_hidden\": [0, 1, 2, 3],\n",
        "\t\"model__n_neurons\": np.arange(1, 100),\n",
        "\t\"model__learning_rate\": reciprocal(3e-4, 3e-2)\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(\n",
        "\testimator=keras_reg,\n",
        "\tparam_distributions=param_distribution_prefixed,\n",
        "\tn_iter=10,\n",
        "\tcv=3,\n",
        "\tverbose=2\n",
        ")\n",
        "rnd_search_cv.fit(\n",
        "\tX_train, y_train,\n",
        "\tepochs=10,\n",
        "\tvalidation_data=(X_valid, y_valid),\n",
        "\tcallbacks=[keras.callbacks.EarlyStopping(patience=5)]\n",
        ")\n",
        "best_params = rnd_search_cv.best_params_\n",
        "best_score = rnd_search_cv.best_score_\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best score:\", best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVztPY_V3UNb"
      },
      "source": [
        "Few libraries for optimizing hyperparameters:\n",
        "\n",
        " Hyperopt:\n",
        "\n",
        " A popular library for optimizing over all sorts of complex search spaces (includ\n",
        "ing real values, such as the learning rate, and discrete values, such as the number\n",
        " of layers).\n",
        "\n",
        " Hyperas, kopt, or Talos:\n",
        "\n",
        " Useful libraries for optimizing hyperparameters for Keras models (the first two\n",
        " are based on Hyperopt).\n",
        "\n",
        " Keras Tuner:\n",
        " An easy-to-use hyperparameter optimization library by Google for Keras models,\n",
        " with a hosted service for visualization and analysis.\n",
        "\n",
        " Scikit-Optimize (skopt):\n",
        " A general-purpose optimization library. The BayesSearchCV class performs\n",
        " Bayesian optimization using an interface similar to GridSearchCV.\n",
        "\n",
        " Spearmint:\n",
        " A Bayesian optimization library.\n",
        "\n",
        " Hyperband:\n",
        " A fast hyperparameter tuning library based on the recent Hyperband paper22 by\n",
        " Lisha Li et al.\n",
        "\n",
        " Sklearn-Deap:\n",
        " A hyperparameter optimization library based on evolutionary algorithms, with a\n",
        " GridSearchCV-like interface\n",
        "\n",
        " The core idea of of these liberaries are: When a region of space turns out to be good, it should be explored more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
